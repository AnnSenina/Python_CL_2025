{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSF5uS39evpMZyIiL4uoh8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnSenina/Python_CL_2025/blob/main/notebooks/%D0%97%D0%B0%D0%B4%D0%B0%D1%87%D0%B8_%D0%BD%D0%B0_%D1%81%D0%B5%D0%BC%D0%B8%D0%BD%D0%B0%D1%80_pandas_(%D1%81_%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D1%8F%D0%BC%D0%B8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 1\n",
        "\n",
        "* Прочитайте текст как строку с помощью .read()\n",
        "* Предобработайте его любым способом (например, с помощью функции с прошлой пары)\n",
        "* Создайте частотный словарь с помощью Counter\n",
        "* Из частотного словаря сформируйте датафрейм pandas, где один столбец - токен, другой - частота его встречаемости (или: где индекс - токен, а столбец - частота встречаемости)\n",
        "\n",
        "Чтобы вас не дезориентировать, первые пункты прописаны, ваша задача - DataFrame"
      ],
      "metadata": {
        "id": "LAWcHqGzswCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('classic.txt', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "q_fy3F4Ft4bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import download\n",
        "# download('punkt')\n",
        "download('punkt_tab') # в Colab нужно скачивать каждый раз\n",
        "download('stopwords') # в Colab нужно скачивать каждый раз\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('russian')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def clean_tokens(text):\n",
        "  text = text.lower()\n",
        "  text_list_nltk = word_tokenize(text)\n",
        "  text_without_punkt = [word for word in text_list_nltk if word[0].isalnum()]\n",
        "  text_clean = [word for word in text_without_punkt if word not in stop_words]\n",
        "  return text_clean"
      ],
      "metadata": {
        "id": "ZczLRhY1t4U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "res = Counter(clean_tokens(text))\n",
        "res"
      ],
      "metadata": {
        "id": "TpCaxpVgt4NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zz_ewKtgsik0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* вариант со звездочкой: можно также не удалять стоп-слова, чтобы проверить, работает ли на этих небольших данных закон Цифпа"
      ],
      "metadata": {
        "id": "eHSBPVEhtsC1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N0aLTSiIt3l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# здесь можно посмотреть на график частот по этому тексту\n",
        "# ascending=False - сортировка по убыванию для чисел\n",
        "# .plot() - забегая вперед, встроенный в pandas кусочек matplotlib\n",
        "df['ваш_столбец'].sort_values(ascending=False).plot();"
      ],
      "metadata": {
        "id": "J_ODbk0YxFKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2\n",
        "\n",
        "* Теперь откроем этот же текст, разрезав на абзацы с помощью .readlines()\n",
        "* Полученный список абзацев превратите в df из одного столбца\n"
      ],
      "metadata": {
        "id": "ETHAr64Ut5UV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AVhBrzejt8sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ViQ6NyhuuYdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вот так можно применить какую-то функцию (стандартную или вашу авторскую) ко всему столбцу:\n",
        "\n",
        "```df['ваш_столбец'].apply(ваша_функция)```"
      ],
      "metadata": {
        "id": "TcQYH9cAuZBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df['ваш_столбец'].apply(len)\n",
        "# df['ваш_столбец'].apply(clean_tokens)\n",
        "# df['ваш_столбец'].apply(lambda x: ' '.join(x))\n",
        "# df['ваш_столбец'].apply(lambda x: re.findall(r'[A-Za-z]+', x))\n",
        "# и др."
      ],
      "metadata": {
        "id": "2mO2A2OtugWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Измените функцию по предобработке так, чтобы она возвращала строку из чистых токенов через пробел с помощью ```' '.join(ваш_список)``` (или предобработайте функцией без изменений, а потом соедините то, что получилось, lambda-функцией)\n",
        "\n",
        "Добавьте в таблицу новый столбец с чистыми токенами"
      ],
      "metadata": {
        "id": "g3ve4qsiuhn4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MP8UuRu2uhPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "вариант со звездочкой: добавьте в таблицу еще 2 новых столбца: сколько наивных токенов было до предобработки и сколько осталось (нужно посчитать длину через len)"
      ],
      "metadata": {
        "id": "APLHHI820Ac4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QUuVfCG30NSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GnzsHMBz0MzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 3"
      ],
      "metadata": {
        "id": "3l_epE1i5lac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(само по себе не очень простое, так что со звездочкой)\n",
        "\n",
        "Давайте еще поработаем с df с пары"
      ],
      "metadata": {
        "id": "fiOtbE4g5rtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recipes = pd.read_csv(\"https://raw.githubusercontent.com/AnnSenina/Python_CL_2024/refs/heads/main/data/christmas_recipes.csv\", sep=',', encoding='utf-8')\n",
        "recipes"
      ],
      "metadata": {
        "id": "JFLptBIb5nrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предобработайте столбцы Ingredients и\tInstructions с помощью apply\n",
        "* Ingredients удалите цифры и пунктуацию (напишите регулярку, которая оставит только сочетания из букв - латиницу), а также слова, длина которых меньше 4 символов (этот код уже написан)\n",
        "* Теперь соедините все ячейки в одну строку с помощью ```recipes['ваш_столбец'].to_list()``` и ```' '.join()``` и надите самый частотные слова (см. подсказку)\n",
        "* После этого найдите топ-10 наиболее частотных слов в рождественских рецептах"
      ],
      "metadata": {
        "id": "uX5GqDgH5vgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "recipes['clean_Ingredients'] = recipes['Ingredients'].apply(lambda x: re.findall(r'здесь_ваша_регулярка', x)).\\\n",
        "  apply(lambda x: ' '.join([word for word in x if len(word) > 3]))\n",
        "recipes['clean_Ingredients'].to_list()"
      ],
      "metadata": {
        "id": "A5BEJtYG5nR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recipes['ваш_столбец'].to_list()\n",
        "# ' '.join(recipes['ваш_столбец'].to_list())\n",
        "# а теперь можно нарезать по пробелу и отправлять в Counter\n",
        "\n"
      ],
      "metadata": {
        "id": "4TwMGAOu6I8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Важно! Без морфологии мы пока не найдем топ-10 наиболее частотных ингридиентов-существительных, можно вернуться к этой задаче после"
      ],
      "metadata": {
        "id": "bAxTQFfG9rp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Instructions просто предобработайте обычным способом (не забудьте загрузить актуальный список стоп-слов на английском из nltk) и найдите топ-10 наиболее частотных слов (по аналогии)"
      ],
      "metadata": {
        "id": "tDi2Q3cX96WR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kubcQXXQ92dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 4\n",
        "\n",
        "Давайте поработаем с твитами Маска и Трампа (скачаны с Kaggle)"
      ],
      "metadata": {
        "id": "NgwZuHb_AQlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "musk = pd.read_csv('https://raw.githubusercontent.com/AnnSenina/Python_CL_2024/refs/heads/main/data/elonmusk.csv', encoding='utf-8')\n",
        "trump = pd.read_csv('https://raw.githubusercontent.com/AnnSenina/Python_CL_2024/refs/heads/main/data/trump.csv', encoding='utf-8')\n",
        "\n",
        "musk_tweets = musk[\"tweet\"] # указываю столбцы, с которыми нужно работать\n",
        "trump_tweets = trump['content']"
      ],
      "metadata": {
        "id": "iAqKz0NGA0Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Предобработайте твиты любым удобным способом, основное - удалить пунктуацию и стоп-слова (english из nltk), в идеале - собрать из них единый текст по аналогии с заданием 3\n",
        "* Найдите топ-30 наиболее частотных биграмм в обоих текстах"
      ],
      "metadata": {
        "id": "auo3EjIABe_y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Img0OmLdBdFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Smr3twWUB19U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Все решения"
      ],
      "metadata": {
        "id": "vuasK7ahuipT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 1"
      ],
      "metadata": {
        "id": "1YtDisPcwVl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "df = pd.DataFrame.from_dict(res, orient=\"index\").rename(columns={0:\"частота\"})\n",
        "df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wbmGFIzTvY0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "df['частота'].sort_values(ascending=False)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WaFiFAE3v465"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# вариант со звездочкой\n",
        "\n",
        "def clean_punct(text):\n",
        "  text = text.lower()\n",
        "  text_list_nltk = word_tokenize(text)\n",
        "  text_without_punkt = [word for word in text_list_nltk if word[0].isalnum()]\n",
        "  #text_clean = [word for word in text_without_punkt if word not in stop_words]\n",
        "  return text_without_punkt\n",
        "\n",
        "res = Counter(clean_punct(text))\n",
        "df = pd.DataFrame.from_dict(res, orient=\"index\").rename(columns={0:\"частота\"})\n",
        "df['частота'].sort_values(ascending=False).plot();"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5YXEDhkvwX_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 2"
      ],
      "metadata": {
        "id": "cwCnMnjByHNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "with open('classic.txt', encoding='utf-8') as f:\n",
        "  text_list = f.readlines()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tdctlzrByG8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "df = pd.DataFrame(text_list, columns=['текст'])\n",
        "df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "spUp-NEmyUUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def clean_tokens(text):\n",
        "  text = text.lower()\n",
        "  text_list_nltk = word_tokenize(text)\n",
        "  text_without_punkt = [word for word in text_list_nltk if word[0].isalnum()]\n",
        "  text_clean = [word for word in text_without_punkt if word not in stop_words]\n",
        "  return ' '.join(text_clean)\n",
        "\n",
        "df['чистые токены'] = df['текст'].apply(clean_tokens)\n",
        "df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SGNGA8sWyfQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# вариант со звездочкой\n",
        "df['количество_наив_токенов'] = df['текст'].apply(lambda x: len(x.split()))\n",
        "df['количество_чист_токенов'] = df['чистые токены'].apply(lambda x: len(x.split()))\n",
        "df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kxrNu-ES0Omp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 3"
      ],
      "metadata": {
        "id": "nLiM17re6Jdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# пошагово\n",
        "list_recipes = recipes['clean_Ingredients'].to_list()\n",
        "text_recipes = ' '.join(list_recipes)\n",
        "list_recipes = text_recipes.split()\n",
        "Counter(list_recipes).most_common(10)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "watH1A9q7t90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "en_stop_words = stopwords.words('english')\n",
        "\n",
        "def clean_tokens(text):\n",
        "  text = text.lower()\n",
        "  text_list_nltk = word_tokenize(text)\n",
        "  text_without_punkt = [word for word in text_list_nltk if word[0].isalnum()]\n",
        "  text_clean = [word for word in text_without_punkt if word not in en_stop_words]\n",
        "  return ' '.join(text_clean)\n",
        "\n",
        "recipes['clean_Instructions'] = recipes['Instructions'].apply(clean_tokens)\n",
        "list_recipes = recipes['clean_Instructions'].to_list()\n",
        "text_recipes = ' '.join(list_recipes)\n",
        "list_recipes = text_recipes.split()\n",
        "Counter(list_recipes).most_common(10)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6N0qRUzU-MTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 4"
      ],
      "metadata": {
        "id": "MRsQFmXQB48j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "def clean_tokens(text):\n",
        "  text = text.lower()\n",
        "  text_list_nltk = word_tokenize(text)\n",
        "  text_without_punkt = [word for word in text_list_nltk if word[0].isalnum()]\n",
        "  text_clean = [word for word in text_without_punkt if word not in en_stop_words]\n",
        "  return ' '.join(text_clean)\n",
        "\n",
        "musk_tweets = musk[\"tweet\"].apply(clean_tokens).to_list()\n",
        "trump_tweets = trump['content'].apply(clean_tokens).to_list()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KUshLRc5B6Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import nltk # для биграмм\n",
        "musk_text = ' '.join(musk_tweets)\n",
        "musk_tokens = musk_text.split()\n",
        "Counter(nltk.bigrams(musk_tokens)).most_common(30)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eLdDTVaIB9yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "trump_text = ' '.join(trump_tweets)\n",
        "trump_tokens = trump_text.split()\n",
        "Counter(nltk.bigrams(trump_tokens)).most_common(30)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9TXhUC15DIQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}